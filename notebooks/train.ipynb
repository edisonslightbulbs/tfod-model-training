{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73a1637-4778-44cb-b474-e6ff766d595b",
   "metadata": {},
   "source": [
    " # Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803f8c23-dd72-4acd-8282-1f29c69a5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "repo = git.Repo(\".\", search_parent_directories=True)\n",
    "rootdir = repo.git.rev_parse(\"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd61366-c6d0-4adb-af81-1e0521149a04",
   "metadata": {},
   "source": [
    " #### Load project parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879d222-76b1-41ca-be1d-b2a554dbb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(rootdir, \"config.json\")\n",
    "with open(path, \"r\") as file:\n",
    "    parameters = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25600708-9eb6-408d-9017-893aeb99c08a",
   "metadata": {},
   "source": [
    " #### Setup work directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8887ae89-5070-4fd2-9b27-50565ad863bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = {'data': os.path.join(rootdir, 'data'),\n",
    "        'labels': os.path.join(rootdir, 'data', 'labels'),\n",
    "        'records': os.path.join(rootdir, 'data', 'records'),\n",
    "        'trained model': os.path.join(rootdir, 'models', 'trained'),\n",
    "        'pretrained model': os.path.join(rootdir, 'models', 'pre-trained'),\n",
    "        'tensorflow': os.path.join(rootdir, 'external','Tensorflow', 'models')}\n",
    "\n",
    "for k,v in dirs.items():\n",
    "    print(f'{k} directory: {v}')\n",
    "    if not os.path.exists(v):\n",
    "        os.makedirs(v, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bd7b62-9a83-4607-9009-7c647cf3f740",
   "metadata": {},
   "source": [
    " #### Create label map and tensorflow records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbfa2e8-e540-4c53-ab4d-108a0f0ce4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# prepare train and test data for an object detection model by converting images\n",
    "# and labels to TFRecord format, which is a binary file format for training\n",
    "# tensorflow models\n",
    "#\n",
    "print(\"generating label map ...\")\n",
    "script = os.path.join(rootdir, 'src',  'labels.py')\n",
    "cmd = f'python {script}'\n",
    "labelmap = os.path.join(dirs['labels'],  'map.pbtxt')\n",
    "os.system(cmd)\n",
    "\n",
    "print(\"splitting test and train data...\")\n",
    "script = os.path.join(rootdir, 'src',  'partition.py')\n",
    "cmd = f'python {script}'\n",
    "os.system(cmd)\n",
    "\n",
    "script = os.path.join(dirs['records'],  'generate.py')\n",
    "\n",
    "print(\"generating train record ...\")\n",
    "images = os.path.join(dirs['data'], 'train')\n",
    "record = os.path.join(dirs['records'], 'train.record')\n",
    "cmd = f'python {script} -x {images} -l {labelmap} -o {record}'\n",
    "os.system(cmd)\n",
    "\n",
    "print(\"generating test record ...\")\n",
    "images = os.path.join(dirs['data'], 'test')\n",
    "record = os.path.join(dirs['records'], 'test.record')\n",
    "cmd = f'python {script} -x {images} -l {labelmap} -o {record}'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa9462-2f4b-4c8e-9cb6-945bb4761a08",
   "metadata": {},
   "source": [
    " #### Copy pipeline configuration of pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b03ec-4b0b-4dcc-ab63-5d7e1c40d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = parameters['pretrained model']['name']\n",
    "src = os.path.join(dirs['pretrained model'], pretrained_model, 'pipeline.config')\n",
    "dst = os.path.join(dirs['trained model'])\n",
    "\n",
    "try:\n",
    "    shutil.copy(src, dst)\n",
    "    os.chmod(dst, 0o666)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f75589-dc5d-4049-9edc-0e4b407b3189",
   "metadata": {},
   "source": [
    " #### Setup pipeline configuration for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12faf439-63b2-4383-9c92-f6d02bbd938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from google.protobuf import text_format\n",
    "from object_detection.protos import pipeline_pb2\n",
    "\n",
    "pipeconfig = os.path.join(dirs['trained model'], 'pipeline.config')\n",
    "assert os.path.exists(pipeconfig), f\"Unable to resolve: {pipeconfig}\"\n",
    "\n",
    "pipeline = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "\n",
    "with tf.io.gfile.GFile(pipeconfig) as file:\n",
    "    proto_str = file.read()\n",
    "    text_format.Merge(proto_str, pipeline)\n",
    "\n",
    "pipeline.model.ssd.num_classes = len(labelmap)\n",
    "test_record = os.path.join(dirs['records'], 'test.record')\n",
    "train_record = os.path.join(dirs['records'], 'train.record')\n",
    "ckpt = parameters['pretrained model']['ckpt']\n",
    "ckpt_path = os.path.join(dirs['pretrained model'], pretrained_model, 'checkpoint', f'ckpt-{ckpt}')\n",
    "\n",
    "pipeline.train_config.batch_size = parameters['batch size']\n",
    "pipeline.train_config.fine_tune_checkpoint = ckpt_path\n",
    "pipeline.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline.train_input_reader.label_map_path= labelmap\n",
    "pipeline.train_input_reader.tf_record_input_reader.input_path[:] = [train_record]\n",
    "\n",
    "pipeline.eval_input_reader[0].label_map_path = labelmap\n",
    "pipeline.eval_input_reader[0].tf_record_input_reader.input_path[:] = [test_record]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c72da2-283d-4f42-92c2-828582d97c2b",
   "metadata": {},
   "source": [
    " #### Check pipeline config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bba0b-c008-4fb6-852b-31e1292e617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# (optional)\n",
    "# \n",
    "# print(text_format.MessageToString(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9e352-5177-4ca8-b9ec-4afef8a3c4d6",
   "metadata": {},
   "source": [
    " #### Write pipeline config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fdc655-ed26-4cd0-ae9c-224faa6d87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.gfile.GFile(pipeconfig, 'wb') as file:\n",
    "    file.write(text_format.MessageToString(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa1cc9d-0e22-434f-b20a-3c55223ffaa9",
   "metadata": {},
   "source": [
    " #### Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1121ae44-b4f2-4907-9c2d-b89c5c1d0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = parameters['epochs']\n",
    "modeldir = dirs['trained model']\n",
    "script = os.path.join(dirs['tensorflow'], 'research', 'object_detection', 'model_main_tf2.py')\n",
    "\n",
    "cmd = f'python {script} --model_dir={modeldir} --pipeline_config_path={pipeconfig} --num_train_steps={epochs}'\n",
    "os.system(cmd)\n",
    "\n",
    "cmd = f'python {script} --model_dir={modeldir} --pipeline_config_path={pipeconfig} --checkpoint_dir={modeldir}'\n",
    "os.system(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
